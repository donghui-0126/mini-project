{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab900f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "# https://github.com/seungeunrho/minimalRL\n",
    "\n",
    "#PPO-LSTM\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20\n",
    "\n",
    "class RECURRENT_PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RECURRENT_PPO, self).__init__()\n",
    "        self.data = []\n",
    "\n",
    "        self.fc1   = nn.Linear(4,64)\n",
    "        self.lstm  = nn.LSTM(64,32)\n",
    "        self.fc_pi = nn.Linear(32,2)\n",
    "        self.fc_pi2   = nn.Linear(3,2)\n",
    "        self.fc_pi3 = nn.Linear(2,2)\n",
    "\n",
    "        self.fc_v  = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, hidden, prob_pre_action_0, prob_pre_action_1):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        x = self.fc_pi(x)\n",
    "        # prob = F.softmax(x, dim=2)\n",
    "\n",
    "        prob = torch.cat([x, torch.tensor(prob_pre_action_0).view(-1,1,1)], -1)\n",
    "        # prob = torch.cat([x], -1)\n",
    "        prob = F.softmax(self.fc_pi2(prob), dim=2)\n",
    "\n",
    "\n",
    "        return prob, lstm_hidden\n",
    "\n",
    "    def v(self, x, hidden):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 1, 64)\n",
    "        x, lstm_hidden = self.lstm(x, hidden)\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, h_in_lst, h_out_lst, done_lst, prob_pre_action_0_lst, prob_pre_action_1_lst = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "        for transition in self.data:\n",
    "            s, a, r, s_prime, prob_a, h_in, h_out, done, prob_pre_action_0, prob_pre_action_1 = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            h_in_lst.append(h_in)\n",
    "            h_out_lst.append(h_out)\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            prob_pre_action_0_lst.append(prob_pre_action_0)\n",
    "            prob_pre_action_1_lst.append(prob_pre_action_1)\n",
    "\n",
    "\n",
    "        s,a,r,s_prime,done_mask,prob_a, prob_pre_action_0, prob_pre_action_1 = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                         torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                         torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst), torch.tensor(prob_pre_action_0_lst), torch.tensor(prob_pre_action_1_lst)\n",
    "        self.data = []\n",
    "        return s,a,r,s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0], prob_pre_action_0, prob_pre_action_1\n",
    "\n",
    "    def train_net(self):\n",
    "        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out), prob_pre_action_0, prob_pre_action_1 = self.make_batch()\n",
    "        first_hidden  = (h1_in.detach(), h2_in.detach())\n",
    "        second_hidden = (h1_out.detach(), h2_out.detach())\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            v_prime = self.v(s_prime, second_hidden).squeeze(1)\n",
    "            td_target = r + gamma * v_prime * done_mask\n",
    "            v_s = self.v(s, first_hidden).squeeze(1)\n",
    "            delta = td_target - v_s\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for item in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + item[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "            pi, _ = self.pi(s, first_hidden, prob_pre_action_0, prob_pre_action_1) # 애초에 h_in이 policy 신경망을 통과했기 때문에 first_hidden을 사용하는 것 같음.\n",
    "            pi_a = pi.squeeze(1).gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == log(exp(a)-exp(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v_s, td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward(retain_graph=True)\n",
    "            self.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
